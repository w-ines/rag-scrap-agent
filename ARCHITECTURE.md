# ğŸ—ï¸ Architecture RAG-Scrap-Agent

## ğŸ“‹ Vue d'ensemble

Le systÃ¨me suit une architecture en 2 Ã©tapes:
1. **`/ask` endpoint** : GÃ¨re uniquement l'upload des fichiers
2. **smolagent** : Prend toutes les dÃ©cisions intelligentes (RAG/summarize/scrape)

---

## ğŸ”„ Flux de traitement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Next.js)                        â”‚
â”‚  User clicks "Ask" with query + optional files              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Next.js API Route (/api/ask)                    â”‚
â”‚  Proxies request to backend                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Backend (/ask)                          â”‚
â”‚                                                              â”‚
â”‚  STEP 1: File Upload Processing (if files present)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ For each file:                             â”‚             â”‚
â”‚  â”‚  1. store_pdf()    â†’ Supabase Storage      â”‚             â”‚
â”‚  â”‚  2. parse_pdf()    â†’ Extract text          â”‚             â”‚
â”‚  â”‚  3. index_documents() â†’ Vector DB          â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                              â”‚
â”‚  STEP 2: Delegate to smolagent                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Query + context â†’ run_agent_sync()         â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SMOLAGENT                                â”‚
â”‚  ğŸ§  Intelligent decision making                             â”‚
â”‚                                                              â”‚
â”‚  Available Tools:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ ğŸ” retrieve_knowledge (RAG)                â”‚             â”‚
â”‚  â”‚    - Searches vector DB for relevant docs  â”‚             â”‚
â”‚  â”‚    - Returns: chunks + metadata            â”‚             â”‚
â”‚  â”‚                                            â”‚             â”‚
â”‚  â”‚ ğŸŒ web_search (Scraping)                   â”‚             â”‚
â”‚  â”‚    - Searches the web via Tavily API       â”‚             â”‚
â”‚  â”‚    - Returns: URLs + snippets              â”‚             â”‚
â”‚  â”‚                                            â”‚             â”‚
â”‚  â”‚ ğŸ•·ï¸ webscraper (Deep scraping)              â”‚             â”‚
â”‚  â”‚    - Scrapes specific URLs                 â”‚             â”‚
â”‚  â”‚    - Returns: full page content            â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                              â”‚
â”‚  Agent reasoning:                                            â”‚
â”‚  1. Analyzes the query                                       â”‚
â”‚  2. Decides which tool(s) to use                            â”‚
â”‚  3. Executes tools in sequence                              â”‚
â”‚  4. Synthesizes final answer                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RESPONSE TO USER                            â”‚
â”‚  { "answer": "..." }                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Exemples de dÃ©cisions de smolagent

### Scenario 1: Question simple sans contexte
```
Query: "what s the fortilla sumud"
Files: None

Agent reasoning:
1. No files uploaded
2. Query doesn't contain URL or scraping keywords
3. Try retrieve_knowledge() first
4. If no results â†’ Could try web_search()
```

### Scenario 2: Upload + Question
```
Query: "summarize this document"
Files: [recipe.pdf]

Agent reasoning:
1. File already uploaded and indexed by /ask
2. Context indicates file just uploaded
3. Use retrieve_knowledge() to get all chunks
4. Synthesize summary from chunks
```

### Scenario 3: Web scraping request
```
Query: "what's on https://example.com/news"
Files: None

Agent reasoning:
1. URL detected in query
2. Use webscraper() to fetch content
3. Summarize the scraped content
```

### Scenario 4: General web search
```
Query: "search the web for latest AI news"
Files: None

Agent reasoning:
1. Keywords "search the web" detected
2. Use web_search() via Tavily API
3. Return top results with sources
```

---

## ğŸ› ï¸ Composants principaux

### 1. `/ask` Endpoint (main.py)
**ResponsabilitÃ©s:**
- âœ… GÃ©rer les uploads multipart
- âœ… Parser et indexer les PDFs
- âœ… Transmettre la query Ã  smolagent
- âŒ **PAS** de logique de dÃ©cision (intent detection supprimÃ©e)

### 2. smolagent (agent.py)
**ResponsabilitÃ©s:**
- âœ… Analyser la query
- âœ… Choisir les outils appropriÃ©s
- âœ… ExÃ©cuter les outils
- âœ… SynthÃ©tiser la rÃ©ponse finale

**Outils disponibles:**
1. `retrieve_knowledge(query)` - RAG search
2. `web_search(query)` - Web search via Tavily
3. `webscraper(url)` - Scrape specific URLs

### 3. Vector Store (vector_store.py)
**ResponsabilitÃ©s:**
- Chunking des documents
- GÃ©nÃ©ration d'embeddings (Ollama)
- Stockage dans Supabase pgvector
- Recherche de similaritÃ©

---

## ğŸ”§ Configuration requise

### Supabase
1. **Table `documents`** avec colonnes:
   - `id` (BIGSERIAL)
   - `content` (TEXT)
   - `metadata` (JSONB)
   - `embedding` (VECTOR(1024))

2. **Fonction `match_documents`**:
```sql
CREATE OR REPLACE FUNCTION match_documents(
  query_embedding VECTOR(1024),
  match_threshold FLOAT DEFAULT 0.5,
  match_count INT DEFAULT 5
)
RETURNS TABLE (...)
```

### Environment Variables
```bash
# Supabase
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=xxx

# Ollama (local)
OLLAMA_EMBED_MODEL=mxbai-embed-large
OLLAMA_BASE_URL=http://localhost:11434

# Tavily (web search)
TAVILY_API_KEY=xxx
```

---

## ğŸ“Š Avantages de cette architecture

1. **SÃ©paration des responsabilitÃ©s**
   - `/ask` = Infrastructure (upload/index)
   - `smolagent` = Intelligence (dÃ©cisions)

2. **FlexibilitÃ©**
   - smolagent peut combiner plusieurs outils
   - Facile d'ajouter de nouveaux outils

3. **SimplicitÃ©**
   - Pas de logique if/else complexe dans `/ask`
   - L'agent raisonne de faÃ§on autonome

4. **TraÃ§abilitÃ©**
   - Logs Ã  chaque Ã©tape
   - Facile de debugger

---

## ğŸš€ Prochaines Ã©tapes

1. âœ… CrÃ©er la fonction `match_documents` dans Supabase
2. âœ… Tester avec une question simple
3. âœ… Tester avec upload + question
4. âœ… Tester avec web scraping
5. â¬œ Ajouter un outil de summarization dÃ©diÃ©
6. â¬œ AmÃ©liorer les prompts de l'agent

